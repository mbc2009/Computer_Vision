{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mbc2009/Inferno/blob/main/AISI/Soln.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y475n-DE0jlO"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mbc2009/Inferno/blob/main/Soln.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUmjdeHY0jlU"
      },
      "source": [
        "**TOC**<a id='toc0_'></a>    \n",
        "1. [Q3](#toc1_)    \n",
        "1.1. [a](#toc1_1_)    \n",
        "1.1.1. [Principle of Operation](#toc1_1_1_)    \n",
        "\n",
        "<!-- vscode-jupyter-toc-config\n",
        "\tnumbering=true\n",
        "\tanchor=true\n",
        "\tflat=true\n",
        "\tminLevel=1\n",
        "\tmaxLevel=6\n",
        "\t/vscode-jupyter-toc-config -->\n",
        "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CFssgeW0jlU"
      },
      "source": [
        "# 1. <a id='toc1_'></a>[Q3](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5M_XVEa0jlV"
      },
      "source": [
        "## 1.1. <a id='toc1_1_'></a>[a](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4szSp74Z0jlV"
      },
      "source": [
        "Basically, at each layer, the GNN/MPNN model will iteratively extract the information from the previous and current layers and execute the following three updating rules,  propagating information from local to global scales to update node and edge representations:\n",
        "\n",
        "**Step 1. Message Generating**<a id=\"eq1\"></a>\n",
        "\n",
        "The message $\\mathbf{m}^{ij,L}$ integrates the state of the sending node $j$ and the receiving node $i$, along with the edge information $\\mathbf{e}^{ij,L-1}$ from their relationship at the previous $(L-1)^{\\text{th}}$ layer.\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\boxed{\n",
        "  \\mathbf{m}^{ij,L}\n",
        "  \\;=\\;\n",
        "  M_L\n",
        "  \\left(\n",
        "    \\mathbf{n}^{i,L-1},\\,\n",
        "    \\mathbf{n}^{j,L-1},\\,\n",
        "    \\mathbf{e}^{ij,L-1}\\;\\;\n",
        "  \\right)\n",
        "}\\\\\n",
        "&\\text{\n",
        "  $\\mathbf{m}^{ij,L}$: Message vector sent from node $j$ to node $i$ at the $L^{\\text{th}}$ layer.}\\\\\n",
        "&\\text{  \n",
        "  $\\mathbf{M_L}$: Learnable message function with parameters specific to layer $L$}\\\\\n",
        "&\\text{  \n",
        "  $\\mathbf{n}^{i,L-1} \\& \\mathbf{n}^{j,L-1}$: State (or feature) vectors of nodes $i$ and $j$ at the previous $(L-1)^{\\text{th}}$ layer.}\\\\\n",
        "&\\text{  \n",
        "  $\\mathbf{e}^{ij,L-1}$: State (or feature) vector of the edge between nodes $i$ and $j$ at the previous $(L-1)^{\\text{th}}$ layer.}\\\\\n",
        "\\end{aligned}\n",
        "\\tag{1}\n",
        "$$\n",
        "$\\mathbf{M_L}$ typically implemented as a neural network (e.g., MLP).\n",
        "\n",
        "**Step 2. Node Updating**\n",
        "\n",
        "The updated node state $\\mathbf{n}^{i,L}$ is computed by aggregating messages from all neighboring nodes $j$ and combining them with the node's previous state $\\mathbf{n}^{i,L-1}$ .\n",
        "Usually, the operator $U_L$ here is a weighted summation followed by an activation function.\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\boxed{\n",
        "  \\mathbf{n}^{i,L}\n",
        "  \\;=\\;\n",
        "  U_L\\!\n",
        "  \\left(\n",
        "    \\mathbf{n}^{i,L-1},\\,\n",
        "    \\sum_{j \\in \\mathcal{N}(i)}\n",
        "    \\mathbf{m}^{ij,L}\\;\\;\n",
        "  \\right)\n",
        "}\\\\\n",
        "&\\text{\n",
        "  $\\mathbf{n}^{i,L}$: Updated state vector of node $i$ at the $L^{\\text{th}}$ layer.}\\\\\n",
        "&\\text{  \n",
        "  $U_L$: Learnable update function for layer $L$, typically a neural network.}\\\\\n",
        "&\\text{  \n",
        "  $\\sum_{j \\in \\mathcal{N}(i)}, \\mathbf{m}^{ij,L}$: Sum of messages from all neighbors $j$ in the neighborhood $\\mathcal{N}(i)$ of node $i$.}\\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "**Step 3. Edge Updating**\n",
        "\n",
        "The current edge state $\\mathbf{e}^{ij,L}$ of the edge between $i$ and $j$, $(i,j)$, is updated based on the current states of the connected nodes $i$ and nodes $j$ and their previous edge state.\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\boxed{\n",
        "  \\mathbf{e}^{ij,L}\n",
        "  \\;=\\;\n",
        "  \\mathcal{N}_L\n",
        "  \\left(\n",
        "      \\mathbf{n}^{i,L},\\,\n",
        "      \\mathbf{n}^{j,L},\\,\n",
        "      \\mathbf{e}^{ij,L-1}\\,\\,\n",
        "  \\right)\n",
        "}\\\\\n",
        "&\\text{\n",
        "  $\\mathbf{e}^{ij,L}$: Updated state vector of the edge between nodes $i$ and $j$ at the $L^{\\text{th}}$ layer.}\\\\\n",
        "&\\text{\n",
        "  $\\mathcal{N}_L$: Learnable edge update function for layer $L$, typically a neural network.}\\\\\n",
        "&\\text{\n",
        "  $\\mathbf{n}^{i,L} \\,\\&\\, \\mathbf{n}^{j,L}$: Current state vectors of nodes $i$ and $j$.}\\\\\n",
        "&\\text{\n",
        "  $\\mathbf{e}^{ij,L-1}$: Previous state vector of the edge at the $(L-1)^{\\text{th}}$ layer.}\\\\\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## b."
      ],
      "metadata": {
        "id": "5UBWXDV0jWhW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the classical GNN/MPNN model, as the depth of the layers increases, each node gradually aggregates (extracts and stores) information from an expanding set of nodes, potentially reaching all others in the graph through multi-hop propagation, *i.e.*, losing its locality.\n",
        "\n",
        "However, for a machine learning model like [DeePTB](https://github.com/deepmodeling/DeePTB), to better preserve its locality and achieve a strictly local receptive field, a new parameter $\\mathbf{V}$ has been introduced into the previous formula to modulate the scope of information propagation.\n",
        "\n",
        "The vertex feature associated with the edge between nodes $i$ and $j$ at layer $L$, *i.e.*, $\\mathbf{V}^{ij,L}$, is iteratively and strictly updated using information aggregated from the master node $i$ itself via:\n",
        "$$\n",
        "\\mathbf{V}^{ij,L}\n",
        "=\n",
        "\\mathcal{V}_L\n",
        "\\left(\n",
        "  \\mathbf{n}^{i,L-1}\\;, \\;\n",
        "  \\mathbf{V}^{ij,L-1} \\;\\;\n",
        "\\right),\n",
        "$$\n",
        "and this information is then passed into $\\mathbf{m}^{ij,L}$ through:\n",
        "$$\n",
        "\\mathbf{m}^{ij,L}\n",
        "=\n",
        "M_L\n",
        "\\left(\n",
        "  \\mathbf{n}^{i,L-1}\\;,\\;\n",
        "  \\mathbf{V}^{ij,L}\\;\\;\n",
        "\\right)\n",
        "$$\n",
        "rather than incorporating the node state of its neighboring node $j$ at the previous layer ($\\mathbf{n}^{j,L-1}$ ), as done in eq.[1](#eq1).\n",
        "\n",
        "This means the information stored in node $i$ no longer contains contributions aggregated from its neighboring nodes $j$, unlike the classical GNN/MPNN model.\n",
        "More specifically, the states of neighboring nodes and connected edge states are not stored in the master node $i$.\n",
        "\n",
        "Instead, this information is collected in the connected edge state $\\mathbf{e}^{ij,L}$ via\n",
        "$$\n",
        "\\mathbf{e}^{ij,L}\n",
        "\\;=\\;\n",
        "\\mathcal{N}_L\n",
        "\\left(\n",
        "  \\mathbf{n}^{i,L},\\,\n",
        "  \\mathbf{V}^{ij,L},\\,\n",
        "  \\mathbf{n}^{j,L},\\,\n",
        "  \\mathbf{e}^{ij,L-1}\\;\\;\n",
        "\\right).\n",
        "$$\n",
        "\n",
        "Therefore, the most significant difference between the classical GNN/MPNN and the modified version used in DeePTB lies in the introduction of the $\\mathbf{V}^{ij,L}$ term, which enforces the accompanying locality."
      ],
      "metadata": {
        "id": "akHZvvCWjYff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# c."
      ],
      "metadata": {
        "id": "VnQAQ1ts9DQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Scalar Assumptions and Linear Transforms**\n",
        "\n",
        "Assume \\(\\mathbf{n}^{i,L}\\), \\(\\mathbf{V}^{ij,L}\\), \\(\\mathbf{m}^{ij,L}\\), and \\(\\mathbf{e}^{ij,L}\\) are all scalars (as allowed by the problem). We then use simple linear transformations for \\(\\mathcal{V}_L\\), \\(M_L\\), \\(U_L\\), and \\(\\mathcal{N}_L\\). For example:\n",
        "- \\(\\mathcal{V}_L(a, b) = w_v a + b\\)\n",
        "- \\(M_L(a, b) = w_m a + b\\)\n",
        "- \\(U_L(a, b) = w_u a + b\\)\n",
        "- \\(\\mathcal{N}_L(a, b, c, d) = w_e (a + b + c + d)\\)\n",
        "\n",
        "Here, \\(w_v, w_m, w_u, w_e\\) are learnable weights.\n",
        "\n",
        "**2. Constructing a Honeycomb Lattice**\n",
        "\n",
        "A honeycomb lattice is a two-dimensional hexagonal grid, where each node has 3 nearest neighbors. We can build a small lattice (e.g., a 4$\\times$4 cell) and use NetworkX to generate the lattice structure.\n",
        "\n",
        "\n",
        "**3. Code Algorithm**\n",
        "\n",
        "1. **Lattice Generation**  \n",
        "   - Use `networkx.hexagonal_lattice_graph` to create a 4×4 honeycomb lattice, where each node has 3 nearest neighbors.  \n",
        "   - Nodes are automatically labeled by NetworkX.\n",
        "\n",
        "2. **Update Formula Implementation**  \n",
        "   - Implement a `DeePTBLayer` class to update \\(\\mathbf{V}^{ij,L}\\), \\(\\mathbf{m}^{ij,L}\\), \\(\\mathbf{n}^{i,L}\\), and \\(\\mathbf{e}^{ij,L}\\).  \n",
        "   - All features are scalars; \\(\\mathcal{V}_L\\), \\(M_L\\), \\(U_L\\), and \\(\\mathcal{N}_L\\) are simplified to linear transformations.\n",
        "\n",
        "3. **Locality Test**  \n",
        "   - Pick a center node \\(i\\) and a distant node \\(k\\) (with distance > 1 so that \\(k\\) is not within \\(i\\)’s nearest neighborhood).  \n",
        "   - **First run**: Initialize every node feature to 1.  \n",
        "   - **Second run**: Set the distant node \\(k\\)’s initial feature to 10.  \n",
        "   - Compare the final feature of the center node \\(i\\) in both runs.  \n",
        "   - Since DeePTB's update formula (single-layer) depends only on nearest neighbors, changes in the distant node \\(k\\) should not affect the center node \\(i\\). This verifies locality.\n",
        "\n",
        "4. **Result**  \n",
        "   - After running the code, the center node \\(i\\) retains the same feature value in both runs (e.g., 1.5, depending on weight initialization).  \n",
        "   - This confirms that the DeePTB update framework is invariant to changes outside the nearest-neighbor range, satisfying strict locality."
      ],
      "metadata": {
        "id": "RkS6MEzVASNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. 构建蜂窝晶格\n",
        "def generate_honeycomb_lattice(rows, cols):\n",
        "    G = nx.hexagonal_lattice_graph(rows, cols, periodic=False)\n",
        "    return G\n",
        "\n",
        "# 2. DeePTB 更新模块\n",
        "class DeePTBLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeePTBLayer, self).__init__()\n",
        "        # 简单线性变换权重\n",
        "        self.w_v = nn.Parameter(torch.tensor(0.5))  # V 更新权重\n",
        "        self.w_m = nn.Parameter(torch.tensor(0.5))  # 消息生成权重\n",
        "        self.w_u = nn.Parameter(torch.tensor(0.5))  # 节点更新权重\n",
        "        self.w_e = nn.Parameter(torch.tensor(0.2))  # 边更新权重\n",
        "\n",
        "    def forward(self, G, node_features, vertex_features, edge_features):\n",
        "        # node_features: [num_nodes], vertex_features: [num_edges], edge_features: [num_edges]\n",
        "        num_nodes = len(G.nodes)\n",
        "        num_edges = len(G.edges)\n",
        "        new_node_features = torch.zeros_like(node_features)\n",
        "        new_vertex_features = torch.zeros_like(vertex_features)\n",
        "        new_edge_features = torch.zeros_like(edge_features)\n",
        "\n",
        "        # 边到索引的映射\n",
        "        edge_index_map = {edge: idx for idx, edge in enumerate(G.edges)}\n",
        "\n",
        "        # Step 1: 更新 vertex feature V_ij,L\n",
        "        for edge in G.edges:\n",
        "            i, j = edge\n",
        "            edge_idx = edge_index_map[edge]\n",
        "            # V_ij,L = V_L(n_i,L-1, V_ij,L-1)\n",
        "            new_vertex_features[edge_idx] = self.w_v * node_features[i] + vertex_features[edge_idx]\n",
        "\n",
        "        # Step 2: 生成消息 m_ij,L\n",
        "        messages = torch.zeros(num_nodes, num_nodes)\n",
        "        for edge in G.edges:\n",
        "            i, j = edge\n",
        "            edge_idx = edge_index_map[edge]\n",
        "            # m_ij,L = M_L(n_i,L-1, V_ij,L)\n",
        "            messages[i, j] = self.w_m * node_features[i] + new_vertex_features[edge_idx]\n",
        "\n",
        "        # Step 3: 更新节点 n_i,L\n",
        "        for i in G.nodes:\n",
        "            # 聚合来自邻居的消息\n",
        "            message_sum = sum(messages[i, j] for j in G.neighbors(i))\n",
        "            # n_i,L = U_L(n_i,L-1, sum(m_ij,L))\n",
        "            new_node_features[i] = self.w_u * node_features[i] + message_sum\n",
        "\n",
        "        # Step 4: 更新边 e_ij,L\n",
        "        for edge in G.edges:\n",
        "            i, j = edge\n",
        "            edge_idx = edge_index_map[edge]\n",
        "            # e_ij,L = N_L(n_i,L, V_ij,L, n_j,L, e_ij,L-1)\n",
        "            new_edge_features[edge_idx] = self.w_e * (\n",
        "                new_node_features[i] + new_vertex_features[edge_idx] +\n",
        "                new_node_features[j] + edge_features[edge_idx]\n",
        "            )\n",
        "\n",
        "        return new_node_features, new_vertex_features, new_edge_features\n",
        "\n",
        "# 3. 测试局部性\n",
        "def test_locality():\n",
        "    # 生成 4x4 蜂窝晶格\n",
        "    G = generate_honeycomb_lattice(4, 4)\n",
        "    num_nodes = len(G.nodes)\n",
        "    num_edges = len(G.edges)\n",
        "\n",
        "    # 初始化特征（标量）\n",
        "    node_features = torch.ones(num_nodes)  # n_i = 1\n",
        "    vertex_features = torch.zeros(num_edges)  # V_ij = 0\n",
        "    edge_features = torch.zeros(num_edges)  # e_ij = 0\n",
        "\n",
        "    # 选择一个中心节点 (i) 和一个远端节点 (k)（超出最近邻范围）\n",
        "    center_node = 10  # 假设中心节点\n",
        "    far_node = 0      # 假设远端节点（通过检查晶格结构确保距离 > 1）\n",
        "\n",
        "    # 层数\n",
        "    num_layers = 1  # 仅测试一层，确保最近邻局部性\n",
        "\n",
        "    # 模型\n",
        "    model = DeePTBLayer()\n",
        "\n",
        "    # 第一次运行：基准\n",
        "    node_features_base = node_features.clone()\n",
        "    vertex_features_base = vertex_features.clone()\n",
        "    edge_features_base = edge_features.clone()\n",
        "    for _ in range(num_layers):\n",
        "        node_features_base, vertex_features_base, edge_features_base = model(\n",
        "            G, node_features_base, vertex_features_base, edge_features_base\n",
        "        )\n",
        "\n",
        "    # 第二次运行：改变远端节点 k 的初始值\n",
        "    node_features_modified = node_features.clone()\n",
        "    node_features_modified[far_node] = 10.0  # 改变远端节点特征\n",
        "    vertex_features_modified = vertex_features.clone()\n",
        "    edge_features_modified = edge_features.clone()\n",
        "    for _ in range(num_layers):\n",
        "        node_features_modified, vertex_features_modified, edge_features_modified = model(\n",
        "            G, node_features_modified, vertex_features_modified, edge_features_modified\n",
        "        )\n",
        "\n",
        "    # 验证中心节点 i 是否不变\n",
        "    print(f\"Center node {center_node} feature (base): {node_features_base[center_node]}\")\n",
        "    print(f\"Center node {center_node} feature (modified): {node_features_modified[center_node]}\")\n",
        "    assert torch.allclose(node_features_base[center_node], node_features_modified[center_node]), \\\n",
        "        \"Center node feature changed, locality not preserved!\"\n",
        "\n",
        "    print(\"Locality test passed: Center node feature is invariant to changes outside the receptive field.\")\n",
        "\n",
        "# 运行测试\n",
        "if __name__ == \"__main__\":\n",
        "    test_locality()\n",
        "\n",
        "这个 POC 代码满足题目要求，成功验证了 DeePTB 公式的局部性！"
      ],
      "metadata": {
        "id": "h2GFFxyAppNS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Markdown Guide",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}